import os
import streamlit as st
from langchain.llms import HuggingFacePipeline
from transformers import pipeline
from langchain.chains import RetrievalQA
from langchain.text_splitter import CharacterTextSplitter
from langchain_community.document_loaders import PyPDFLoader
from langchain.vectorstores import FAISS
from langchain.embeddings.huggingface import HuggingFaceEmbeddings
from langchain.prompts import PromptTemplate
from langchain.chains.question_answering import load_qa_chain

# Hugging Face API token
os.environ["HUGGINGFACEHUB_API_TOKEN"] = " API KEY "

st.title("ðŸ“„ PDF RAG Chatbot (MapReduce, Hugging Face)")

pdf_file = st.file_uploader("Bir PDF yÃ¼kleyin", type="pdf")

if pdf_file:
    temp_file_path = "temp.pdf"
    with open(temp_file_path, "wb") as f:
        f.write(pdf_file.getbuffer())

    # PDF yÃ¼kleme
    loader = PyPDFLoader(temp_file_path)
    documents = loader.load()

    # Metni kÃ¼Ã§Ã¼k parÃ§alara ayÄ±r
    text_splitter = CharacterTextSplitter(chunk_size=300, chunk_overlap=50)
    texts = text_splitter.split_documents(documents)


    # Embeddings ve FAISS veritabanÄ±
    embeddings = HuggingFaceEmbeddings()
    db = FAISS.from_documents(texts, embeddings)
    retriever = db.as_retriever()

    # Hugging Face LLM pipeline
    pipe = pipeline(
        "text-generation",
        model="google/flan-t5-small",
        max_length=150,
        do_sample=False
    )
    llm = HuggingFacePipeline(pipeline=pipe)

    # Map prompt (her parÃ§a iÃ§in)
    map_prompt = PromptTemplate(
        template="""
ParÃ§ayÄ± oku ve soruyu kÄ±sa ve net cevapla.
EÄŸer cevap metinde yoksa 'Bilmiyorum' yaz.

ParÃ§a:
{context}

Soru: {question}
Cevap:
""",
        input_variables=["context", "question"]
    )

    # Reduce prompt (tÃ¼m parÃ§alarÄ±n Ã¶zetleri iÃ§in)
    combine_prompt = PromptTemplate(
        template="""
AÅŸaÄŸÄ±daki Ã¶zetleri kullanarak soruyu kÄ±sa ve net ÅŸekilde yanÄ±tla.
EÄŸer cevap yoksa 'Bilmiyorum' yaz.

Ã–zetler:
{summaries}

Soru: {question}
Cevap:
""",
        input_variables=["summaries", "question"]
    )

    # map_reduce zinciri oluÅŸtur
    qa_chain = load_qa_chain(
        llm=llm,
        chain_type="map_reduce",
        question_prompt=map_prompt,
        combine_prompt=combine_prompt
    )

    # RetrievalQA zinciri
    qa = RetrievalQA(
        retriever=retriever,
        combine_documents_chain=qa_chain
    )

    # KullanÄ±cÄ± sorusu
    query = st.text_input("Sorunuzu yazÄ±n:")

    if query:
        answer = qa.run(query)
        st.write("ðŸ¤– Cevap:", answer)
